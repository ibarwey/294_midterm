{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Implementing CNN"
      ],
      "metadata": {
        "id": "exe2a0yAc81p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description"
      ],
      "metadata": {
        "id": "nuE5jFMfWeH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I utilized the CIFAR-10 dataset and a tutorial for CNNs for this dataset (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) in order to accomplish this task. Below are the different experimentats I performed both across hyperparameters and network architectures.\n",
        "\n",
        "1.   Learning Rates\n",
        "\n",
        "* lr=0.1 -> 10 %\n",
        "* lr=0.01 -> 28 %\n",
        "* **lr=0.001 -> 39 %**\n",
        "\n",
        "2.   Epochs\n",
        "\n",
        "* epochs: 2 -> 39 %\n",
        "* epochs: 3 -> 42 %\n",
        "* epochs: 4 ->  57 % %\n",
        "* **epochs: 5 -> 59 %**\n",
        "\n",
        "3.   Fully-connected Layers\n",
        "\n",
        "* I attempted two architectures for fully-connected layers, one as follows:\n",
        "\n",
        "* self.fc1 = nn.Linear(16 * 5 * 5, 200)\n",
        "\n",
        "* self.fc2 = nn.Linear(200, 120)\n",
        "\n",
        "* self.fc3 = nn.Linear(120, 84)\n",
        "\n",
        "* self.fc4 = nn.Linear(84, 10)\n",
        "\n",
        "* accuracy : 54 %\n",
        "\n",
        "* next attempt:\n",
        "\n",
        "* .fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "* self.fc2 = nn.Linear(120, 84)\n",
        "* self.fc3 = nn.Linear(84, 10)\n",
        "* **accuracy : 59 %**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Ultimately, I concluded that the optimal combination for performance was:\n",
        "* 5 epochs\n",
        "* learning rate of 0.001\n",
        "* 3 fully-connected layers"
      ],
      "metadata": {
        "id": "2a5-9j06WYF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "RAV7QkquWili"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5s6EGcChoi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb59179e-a2a8-4d15-de73-9e0f0807d617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# download / normalize data\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define CNN\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "2PSoTeCzAKZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "# train network\n",
        "\n",
        "# STEP 1: experimentations on different hyperparameters:\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQdO1_M-BAzJ",
        "outputId": "0629a65d-e613-401a-dcc2-621fe8894564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.251\n",
            "[1,  4000] loss: 1.947\n",
            "[1,  6000] loss: 1.730\n",
            "[1,  8000] loss: 1.625\n",
            "[1, 10000] loss: 1.545\n",
            "[1, 12000] loss: 1.477\n",
            "[2,  2000] loss: 1.422\n",
            "[2,  4000] loss: 1.372\n",
            "[2,  6000] loss: 1.367\n",
            "[2,  8000] loss: 1.332\n",
            "[2, 10000] loss: 1.298\n",
            "[2, 12000] loss: 1.274\n",
            "[3,  2000] loss: 1.217\n",
            "[3,  4000] loss: 1.224\n",
            "[3,  6000] loss: 1.210\n",
            "[3,  8000] loss: 1.182\n",
            "[3, 10000] loss: 1.205\n",
            "[3, 12000] loss: 1.182\n",
            "[4,  2000] loss: 1.102\n",
            "[4,  4000] loss: 1.110\n",
            "[4,  6000] loss: 1.144\n",
            "[4,  8000] loss: 1.091\n",
            "[4, 10000] loss: 1.108\n",
            "[4, 12000] loss: 1.128\n",
            "[5,  2000] loss: 1.022\n",
            "[5,  4000] loss: 1.048\n",
            "[5,  6000] loss: 1.041\n",
            "[5,  8000] loss: 1.087\n",
            "[5, 10000] loss: 1.042\n",
            "[5, 12000] loss: 1.062\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload saved network\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3FW375ZOYmx",
        "outputId": "9a3ef373-d732-451b-fc8f-94481a61a0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test on whole dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcFkSBGYOd6v",
        "outputId": "a49dd91c-7ecc-468b-8835-26f063d6a0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 59 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Considering MEC"
      ],
      "metadata": {
        "id": "scQyI0LjSCMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description"
      ],
      "metadata": {
        "id": "uZ6FPXYGWs3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order the calulate the minimum MEC needed for a CNN of this dataset, we can use the following formulae:\n",
        "\n",
        "> batch_size * channel * width * height\n",
        "\n",
        "> output = input / compression ratio\n",
        "\n",
        "Applying this to our specific case, we get:\n",
        "\n",
        "> (4 * 3 * 32 * 32) / (192/25) = **1600 minimum MEC**\n",
        "\n",
        "So, 1600 is the desired MEC for the function. Below, I have calculated the MEC for my current implementation based on similar calculations from part 8.1.\n",
        "\n",
        "> (400 * 120) + min(120 * 84, 84) + min(84 * 10, 10) = **48,094 current MEC**\n",
        "\n",
        "The current MEC of my implementation is much larger than it should be. This indicates that I can find ways to reduce or readjust the architecture in my CNN to reduce the MEC to a value closer to 1600, and this should maintain a similar accuracy to the 59% from before. I attempt exactly this by making my new CNN only contain one single fully-connected layer instead of 3. The new architecture looks as follows:\n",
        "\n",
        "> self.fc1 = nn.Linear(16 * 5 * 5, 10)\n",
        "\n",
        "Essentially, the layer reduces from 400 to 10. My new MEC would then be:\n",
        "\n",
        "> 400 * 10 + min(400 * 10, 10) = **4,010 new MEC**\n",
        "\n",
        "While this new MEC is still quite a bit larger than 1600, it is much closer than the previous solution, and it reduces a significant amount of memory usage by only employing one linear layer.\n",
        "\n",
        "After training on the same hyperparameters as the previous model, this model achieved a **57% accuracy**, which is quite similar to the original while reducing memory usage by a large amount."
      ],
      "metadata": {
        "id": "yIyJYYB6Wd6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "EBP8KmDYWviJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing all but 1 fully-connected layer\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class New_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        #reducing to one linear layer reducing from 400 to 10\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "net = New_Net()"
      ],
      "metadata": {
        "id": "ciH--2HNXtrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#running on 5 epochs and learning rate 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "PATH = './cifar_net_new.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjcDAleSYoQL",
        "outputId": "c4a56ba5-f236-4737-9ec5-6b743317db78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 1.983\n",
            "[1,  4000] loss: 1.698\n",
            "[1,  6000] loss: 1.571\n",
            "[1,  8000] loss: 1.506\n",
            "[1, 10000] loss: 1.454\n",
            "[1, 12000] loss: 1.424\n",
            "[2,  2000] loss: 1.360\n",
            "[2,  4000] loss: 1.368\n",
            "[2,  6000] loss: 1.321\n",
            "[2,  8000] loss: 1.323\n",
            "[2, 10000] loss: 1.308\n",
            "[2, 12000] loss: 1.319\n",
            "[3,  2000] loss: 1.227\n",
            "[3,  4000] loss: 1.267\n",
            "[3,  6000] loss: 1.232\n",
            "[3,  8000] loss: 1.240\n",
            "[3, 10000] loss: 1.232\n",
            "[3, 12000] loss: 1.231\n",
            "[4,  2000] loss: 1.159\n",
            "[4,  4000] loss: 1.208\n",
            "[4,  6000] loss: 1.171\n",
            "[4,  8000] loss: 1.182\n",
            "[4, 10000] loss: 1.198\n",
            "[4, 12000] loss: 1.181\n",
            "[5,  2000] loss: 1.168\n",
            "[5,  4000] loss: 1.144\n",
            "[5,  6000] loss: 1.160\n",
            "[5,  8000] loss: 1.169\n",
            "[5, 10000] loss: 1.146\n",
            "[5, 12000] loss: 1.156\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_net = New_Net()\n",
        "new_net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#outputs = new_net(images)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = new_net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "IIccKXz6YzSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HH6Y5feq8A0m",
        "N-Fia1lO8YpB",
        "wOMI2CAkQ8bu",
        "i4Ionmaa89Cy",
        "OpzSmHHR9AQ0",
        "tCXcrdyiRDpQ"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}